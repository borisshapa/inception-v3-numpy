{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch import nn\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets.cars import load_dataset\n",
    "from datasets.cars_torch_dataset import Cars\n",
    "from torch_ada_smooth import AdaSmooth\n",
    "from torch_modules.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "log_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Cars()\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 800, 800], generator=torch.Generator().manual_seed(21))\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "data_loaders = (train_data_loader, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(data_loaders: Tuple[DataLoader, DataLoader], optimizer_type: str = \"ada_smooth\", epochs: int = 1,\n",
    "          lr: float = 1e-2, log_every: int = 50, device: str = \"cuda\"):\n",
    "    train_data_loader, test_data_loader = data_loaders\n",
    "\n",
    "    model = InceptionV3(num_classes=196)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    if optimizer_type == \"ada_smooth\":\n",
    "        optimizer = AdaSmooth(model.parameters(), learning_rate=lr, epsilon=1e-6, slow_decay=0.99, fast_decay=0.5)\n",
    "    elif optimizer_type == \"adam\":\n",
    "        optimizer = Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        print(f\"Unsupported optimizer type: {optimizer_type}\")\n",
    "        return\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    loss_history = []\n",
    "    accuracy = []\n",
    "    precision_micro = []\n",
    "    recall_micro = []\n",
    "    f1_micro = []\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for images, labels in tqdm(train_data_loader, total=len(train_data_loader)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.long().to(device)\n",
    "\n",
    "            predictions, _ = model(images)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            if step % log_every == 0:\n",
    "                model.eval()\n",
    "                predictions, gt = [], []\n",
    "                for images, labels in test_data_loader:\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    pred, _ = model(images)\n",
    "                    predictions.append(pred.argmax(axis=1).cpu().detach())\n",
    "                    gt.append(labels.cpu().detach())\n",
    "\n",
    "                predictions = torch.cat(predictions).numpy()\n",
    "                gt = torch.cat(gt).numpy()\n",
    "\n",
    "                accuracy.append(accuracy_score(gt, predictions))\n",
    "                precision_micro.append(precision_score(gt, predictions, average=\"micro\"))\n",
    "                recall_micro.append(recall_score(gt, predictions, average=\"micro\"))\n",
    "                f1_micro.append(f1_score(gt, predictions, average=\"micro\"))\n",
    "                model.train()\n",
    "        \n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "            f, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3)\n",
    "            f.set_figwidth(15)\n",
    "            f.set_figheight(10)\n",
    "            \n",
    "            ax1.set_title(\"training loss\")\n",
    "            ax2.set_title(\"accuracy\")\n",
    "            ax3.set_title(\"precision micro\")\n",
    "            ax4.set_title(\"recall micro\")\n",
    "            ax5.set_title(\"f1 micro\")\n",
    "\n",
    "            ax1.plot(loss_history)\n",
    "            ax2.plot(accuracy)\n",
    "            ax3.plot(precision_micro)\n",
    "            ax4.plot(recall_micro)\n",
    "            ax5.plot(f1_micro)\n",
    "\n",
    "            plt.show()\n",
    "            print(f\"Current loss: {loss_history[-1]}\")\n",
    "            if len(accuracy) > 0:\n",
    "                print(f\"Current accuracy: {accuracy[-1]}\")\n",
    "                print(f\"Current precision: {precision_micro[-1]}\")\n",
    "                print(f\"Current recall: {recall_micro[-1]}\")\n",
    "                print(f\"Current f1: {f1_micro[-1]}\")\n",
    "    return loss_history, accuracy, precision_micro, recall_micro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "adam_lh, adam_a, adam_p, adam_r, adam_f1 = train(data_loaders, \"adam\", epochs = epochs, lr = lr, log_every = log_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "as_lh, as_a, as_p, as_r, as_f1 = train(data_loaders, \"ada_smooth\", epochs = epochs, lr = lr, log_every = log_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}